{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5540cfbb",
      "metadata": {
        "id": "5540cfbb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import text\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e596b22c",
      "metadata": {
        "id": "e596b22c"
      },
      "outputs": [],
      "source": [
        "#taking random sentences as data\n",
        "data = \"\"\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
        "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
        "\"\"\"\n",
        "dl_data = data.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ae494900",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae494900",
        "outputId": "244cca6f-a048-4ad6-fb65-4936fc1e3449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 75\n",
            "Vocabulary Sample: [('learning', 1), ('deep', 2), ('networks', 3), ('neural', 4), ('and', 5), ('as', 6), ('of', 7), ('machine', 8), ('supervised', 9), ('have', 10)]\n"
          ]
        }
      ],
      "source": [
        "#tokenization\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(dl_data)\n",
        "word2id = tokenizer.word_index\n",
        "\n",
        "word2id['PAD'] = 0\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "embed_size = 100\n",
        "window_size = 2\n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8d236309",
      "metadata": {
        "id": "8d236309"
      },
      "outputs": [],
      "source": [
        "#generating (context word, target/label word) pairs\n",
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "    context_length = window_size*2\n",
        "    for words in corpus:\n",
        "        sentence_length = len(words)\n",
        "        for index, word in enumerate(words):\n",
        "            context_words = []\n",
        "            label_word   = []\n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "\n",
        "            context_words.append([words[i]\n",
        "                                 for i in range(start, end)\n",
        "                                 if 0 <= i < sentence_length\n",
        "                                 and i != index])\n",
        "            label_word.append(word)\n",
        "\n",
        "            x = pad_sequences(context_words, maxlen=context_length)\n",
        "            y = to_categorical(label_word, vocab_size)\n",
        "            yield (x, y)\n",
        "\n",
        "i = 0\n",
        "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "    if 0 not in x[0]:\n",
        "        # print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
        "\n",
        "        if i == 10:\n",
        "            break\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "449d4937",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "449d4937",
        "outputId": "6c85cb91-a811-40ef-953c-4cda7a6ea293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)               â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)               â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "#model building\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "\n",
        "cbow = Sequential()\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "print(cbow.summary())\n",
        "\n",
        "# from IPython.display import SVG\n",
        "# from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "# SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "99dac9a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99dac9a3",
        "outputId": "f2b8746f-b405-4811-80d4-c6906e8e918f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tLoss: 431.32727\n",
            "\n",
            "Epoch: 2 \tLoss: 430.7813\n",
            "\n",
            "Epoch: 3 \tLoss: 429.0235\n",
            "\n",
            "Epoch: 4 \tLoss: 427.27524\n",
            "\n",
            "Epoch: 5 \tLoss: 425.75864\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, 6):\n",
        "    loss = 0.\n",
        "    i = 0\n",
        "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "        i += 1\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "        if i % 100000 == 0:\n",
        "            print('Processed {} (context, word) pairs'.format(i))\n",
        "\n",
        "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "accf5b32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "accf5b32",
        "outputId": "7cf221cf-75aa-44b3-9722-1ea93e3e3f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(74, 100)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                0         1         2         3         4         5   \\\n",
              "deep     -0.001380 -0.052387 -0.020369 -0.003171  0.006323 -0.009616   \n",
              "networks -0.051292  0.000265  0.023165  0.000180  0.030213 -0.014857   \n",
              "neural   -0.006556 -0.006218 -0.013444 -0.015961  0.016594  0.021871   \n",
              "and      -0.043231  0.035912 -0.008876  0.030779 -0.040348  0.014713   \n",
              "as       -0.036716 -0.014763  0.040019  0.040291  0.006574  0.045934   \n",
              "\n",
              "                6         7         8         9   ...        90        91  \\\n",
              "deep      0.023566 -0.011275 -0.021994 -0.032643  ...  0.001856 -0.005692   \n",
              "networks  0.010475  0.032862 -0.019126  0.033064  ... -0.037537 -0.013482   \n",
              "neural    0.012963 -0.034647 -0.026203  0.010552  ... -0.003700  0.043730   \n",
              "and       0.027900 -0.043834 -0.011921 -0.035915  ... -0.035442  0.023745   \n",
              "as       -0.045928  0.035303  0.014310  0.012535  ... -0.028998 -0.001662   \n",
              "\n",
              "                92        93        94        95        96        97  \\\n",
              "deep      0.031364 -0.064365  0.028500 -0.002178  0.049791 -0.057154   \n",
              "networks -0.065375  0.010949  0.031635  0.027153 -0.014851 -0.060854   \n",
              "neural   -0.005246 -0.014007  0.012131 -0.035186  0.029934 -0.009042   \n",
              "and      -0.001483  0.038473 -0.021467  0.019912  0.000988  0.010163   \n",
              "as        0.034958 -0.044080  0.005235 -0.017087 -0.011756  0.013145   \n",
              "\n",
              "                98        99  \n",
              "deep     -0.004367 -0.017154  \n",
              "networks  0.016581 -0.048326  \n",
              "neural   -0.042141  0.008936  \n",
              "and       0.044667  0.001026  \n",
              "as       -0.018816 -0.035061  \n",
              "\n",
              "[5 rows x 100 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a74b0d5d-5cf9-4922-b6e0-9441ae96f0e0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>deep</th>\n",
              "      <td>-0.001380</td>\n",
              "      <td>-0.052387</td>\n",
              "      <td>-0.020369</td>\n",
              "      <td>-0.003171</td>\n",
              "      <td>0.006323</td>\n",
              "      <td>-0.009616</td>\n",
              "      <td>0.023566</td>\n",
              "      <td>-0.011275</td>\n",
              "      <td>-0.021994</td>\n",
              "      <td>-0.032643</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001856</td>\n",
              "      <td>-0.005692</td>\n",
              "      <td>0.031364</td>\n",
              "      <td>-0.064365</td>\n",
              "      <td>0.028500</td>\n",
              "      <td>-0.002178</td>\n",
              "      <td>0.049791</td>\n",
              "      <td>-0.057154</td>\n",
              "      <td>-0.004367</td>\n",
              "      <td>-0.017154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>networks</th>\n",
              "      <td>-0.051292</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.023165</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.030213</td>\n",
              "      <td>-0.014857</td>\n",
              "      <td>0.010475</td>\n",
              "      <td>0.032862</td>\n",
              "      <td>-0.019126</td>\n",
              "      <td>0.033064</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.037537</td>\n",
              "      <td>-0.013482</td>\n",
              "      <td>-0.065375</td>\n",
              "      <td>0.010949</td>\n",
              "      <td>0.031635</td>\n",
              "      <td>0.027153</td>\n",
              "      <td>-0.014851</td>\n",
              "      <td>-0.060854</td>\n",
              "      <td>0.016581</td>\n",
              "      <td>-0.048326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neural</th>\n",
              "      <td>-0.006556</td>\n",
              "      <td>-0.006218</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>0.016594</td>\n",
              "      <td>0.021871</td>\n",
              "      <td>0.012963</td>\n",
              "      <td>-0.034647</td>\n",
              "      <td>-0.026203</td>\n",
              "      <td>0.010552</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003700</td>\n",
              "      <td>0.043730</td>\n",
              "      <td>-0.005246</td>\n",
              "      <td>-0.014007</td>\n",
              "      <td>0.012131</td>\n",
              "      <td>-0.035186</td>\n",
              "      <td>0.029934</td>\n",
              "      <td>-0.009042</td>\n",
              "      <td>-0.042141</td>\n",
              "      <td>0.008936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>-0.043231</td>\n",
              "      <td>0.035912</td>\n",
              "      <td>-0.008876</td>\n",
              "      <td>0.030779</td>\n",
              "      <td>-0.040348</td>\n",
              "      <td>0.014713</td>\n",
              "      <td>0.027900</td>\n",
              "      <td>-0.043834</td>\n",
              "      <td>-0.011921</td>\n",
              "      <td>-0.035915</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.035442</td>\n",
              "      <td>0.023745</td>\n",
              "      <td>-0.001483</td>\n",
              "      <td>0.038473</td>\n",
              "      <td>-0.021467</td>\n",
              "      <td>0.019912</td>\n",
              "      <td>0.000988</td>\n",
              "      <td>0.010163</td>\n",
              "      <td>0.044667</td>\n",
              "      <td>0.001026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>as</th>\n",
              "      <td>-0.036716</td>\n",
              "      <td>-0.014763</td>\n",
              "      <td>0.040019</td>\n",
              "      <td>0.040291</td>\n",
              "      <td>0.006574</td>\n",
              "      <td>0.045934</td>\n",
              "      <td>-0.045928</td>\n",
              "      <td>0.035303</td>\n",
              "      <td>0.014310</td>\n",
              "      <td>0.012535</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028998</td>\n",
              "      <td>-0.001662</td>\n",
              "      <td>0.034958</td>\n",
              "      <td>-0.044080</td>\n",
              "      <td>0.005235</td>\n",
              "      <td>-0.017087</td>\n",
              "      <td>-0.011756</td>\n",
              "      <td>0.013145</td>\n",
              "      <td>-0.018816</td>\n",
              "      <td>-0.035061</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 100 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a74b0d5d-5cf9-4922-b6e0-9441ae96f0e0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a74b0d5d-5cf9-4922-b6e0-9441ae96f0e0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a74b0d5d-5cf9-4922-b6e0-9441ae96f0e0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-636630d5-d56c-40d9-b976-f5aabf5c7577\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-636630d5-d56c-40d9-b976-f5aabf5c7577')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-636630d5-d56c-40d9-b976-f5aabf5c7577 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "weights = cbow.get_weights()[0]\n",
        "weights = weights[1:]\n",
        "print(weights.shape)\n",
        "\n",
        "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "\n",
        "## ğŸ”¶ PRACTICAL NAME:\n",
        "\n",
        "**â€œImplement the Continuous Bag of Words (CBOW) for text recognition in a given dataset.â€**\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¹ OVERVIEW / THEORY OF CBOW\n",
        "\n",
        "### What is CBOW?\n",
        "\n",
        "CBOW (Continuous Bag of Words) is a **word embedding technique** proposed in the **Word2Vec** model by **Mikolov et al. (2013)**.\n",
        "\n",
        "Itâ€™s a **neural network model** that learns to **predict a target word given its surrounding context words**.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”¸ Core Concept:\n",
        "\n",
        "* **Input:** Context words (surrounding words)\n",
        "* **Output:** Target word (the word that fits best in that context)\n",
        "\n",
        "Example:\n",
        "Sentence â†’ â€œThe cat sits on the matâ€\n",
        "If window size = 2, and target = â€œsitsâ€,\n",
        "context words = [â€œTheâ€, â€œcatâ€, â€œonâ€, â€œtheâ€]\n",
        "\n",
        "CBOW tries to **predict \"sits\"** from these context words.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”¸ Working Mechanism (Step-by-Step):\n",
        "\n",
        "1. **Text Preprocessing:**\n",
        "\n",
        "   * Tokenize text (convert words â†’ integers)\n",
        "   * Create vocabulary\n",
        "\n",
        "2. **Context & Target Generation:**\n",
        "\n",
        "   * For each word, select its neighboring words (context)\n",
        "   * Example (window = 2):\n",
        "     Text: â€œThe quick brown fox jumpsâ€\n",
        "     Pairs:\n",
        "     (â€œTheâ€, â€œbrownâ€) â†’ â€œquickâ€\n",
        "     (â€œquickâ€, â€œfoxâ€) â†’ â€œbrownâ€\n",
        "     etc.\n",
        "\n",
        "3. **Model Architecture:**\n",
        "\n",
        "   * **Input layer:** Context words (one-hot encoded)\n",
        "   * **Hidden layer:** Dense layer with embedding dimension\n",
        "   * **Output layer:** Softmax layer to predict target word\n",
        "\n",
        "4. **Training Objective:**\n",
        "\n",
        "   * The model learns **word vectors (embeddings)** that make semantically similar words closer in vector space.\n",
        "   * **Loss:** Cross-entropy loss\n",
        "   * **Optimizer:** SGD / Adam\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”¸ Applications of CBOW:\n",
        "\n",
        "* Word embeddings for NLP\n",
        "* Text classification\n",
        "* Sentiment analysis\n",
        "* Machine translation\n",
        "* Speech/text recognition\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”¸ CBOW vs Skip-Gram:\n",
        "\n",
        "| Feature   | CBOW                     | Skip-Gram           |\n",
        "| --------- | ------------------------ | ------------------- |\n",
        "| Predicts  | Target word from context | Context from target |\n",
        "| Training  | Faster                   | Slower              |\n",
        "| Good for  | Frequent words           | Rare words          |\n",
        "| Direction | Context â†’ Target         | Target â†’ Context    |\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¶ CODE EXPLANATION (CELL BY CELL)\n",
        "\n",
        "### ğŸ§© **Cell 1: Import Libraries**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing import text\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "```\n",
        "\n",
        "**Purpose:**\n",
        "Imports tools for text preprocessing, sequence padding, and categorical encoding.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* `text` â€“ for tokenizing sentences\n",
        "* `sequence` & `pad_sequences` â€“ make all input sequences of equal length\n",
        "* `to_categorical` â€“ converts word indices into one-hot encoded form\n",
        "* `numpy`, `pandas` â€“ used for handling data arrays\n",
        "\n",
        "**Possible Viva Questions:**\n",
        "\n",
        "* Why use `pad_sequences`?\n",
        "* What is one-hot encoding?\n",
        "* Why is text tokenization important?\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§© **Cell 2: Dataset Creation**\n",
        "\n",
        "```python\n",
        "#taking random sentences as data\n",
        "data = \"\"\"Deep learning (also known as deep structured learning) is part of a broader family...\n",
        "\"\"\"\n",
        "dl_data = data.split()\n",
        "```\n",
        "\n",
        "**Purpose:**\n",
        "Creates a simple custom dataset (manually written text).\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* Multiline string â†’ acts as corpus\n",
        "* `split()` â†’ breaks into individual words\n",
        "\n",
        "**Viva Questions:**\n",
        "\n",
        "* Why did you create a custom dataset?\n",
        "* How does `.split()` tokenize the data?\n",
        "* What could be an alternative tokenizer?\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§© **Cell 3: Tokenization and Vocabulary Creation**\n",
        "\n",
        "```python\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(dl_data)\n",
        "word2idx = tokenizer.word_index\n",
        "V = len(word2idx)\n",
        "```\n",
        "\n",
        "**Purpose:**\n",
        "Converts text into numerical format.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* `Tokenizer()` â†’ assigns each unique word an integer\n",
        "* `fit_on_texts()` â†’ builds vocabulary\n",
        "* `word_index` â†’ dictionary {word: index}\n",
        "* `V` â†’ vocabulary size\n",
        "\n",
        "**Viva Questions:**\n",
        "\n",
        "* What is vocabulary size?\n",
        "* Why do we convert words into numbers?\n",
        "* What is the role of `Tokenizer`?\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§© **Cell 4: Creating Context-Target Pairs**\n",
        "\n",
        "```python\n",
        "window_size = 2\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(window_size, len(dl_data) - window_size):\n",
        "    context = []\n",
        "    for j in range(-window_size, window_size + 1):\n",
        "        if j != 0:\n",
        "            context.append(tokenizer.texts_to_sequences([dl_data[i + j]])[0][0])\n",
        "    dataX.append(context)\n",
        "    dataY.append(tokenizer.texts_to_sequences([dl_data[i]])[0][0])\n",
        "```\n",
        "\n",
        "**Purpose:**\n",
        "Generates input (context) and output (target) pairs for CBOW training.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* Loop over words in corpus\n",
        "* Select 2 words before and after as context\n",
        "* Skip the target word itself\n",
        "* Convert each word into its integer ID\n",
        "\n",
        "**Viva Questions:**\n",
        "\n",
        "* What is a context window?\n",
        "* How do you choose window size?\n",
        "* Why exclude the center word?\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§© **Cell 5: Convert Data to NumPy Arrays**\n",
        "\n",
        "```python\n",
        "dataX = np.array(dataX)\n",
        "dataY = np.array(dataY)\n",
        "```\n",
        "\n",
        "Simple conversion for training compatibility.\n",
        "\n",
        "**Viva Question:**\n",
        "\n",
        "* Why use NumPy arrays instead of lists?\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§© **Cell 6: Model Building**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Lambda, Dense\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(V+1, 10, input_length=window_size*2))\n",
        "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(10,)))\n",
        "model.add(Dense(V, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "```\n",
        "\n",
        "**Purpose:**\n",
        "Implements the CBOW neural architecture.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* `Embedding(V+1, 10)` â†’ word embedding layer (10-dimensional vectors)\n",
        "* `Lambda(...mean...)` â†’ averages context embeddings\n",
        "* `Dense(V, softmax)` â†’ predicts target word probability\n",
        "* `compile()` â†’ defines optimizer & loss function\n",
        "\n",
        "**Viva Questions:**\n",
        "\n",
        "* What does the Embedding layer do?\n",
        "* Why use `Lambda` with `K.mean()`?\n",
        "* Why choose â€˜softmaxâ€™ at the output?\n",
        "* What loss function is used in CBOW?\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§© **Cell 7: Prepare Outputs for Training**\n",
        "\n",
        "```python\n",
        "y = to_categorical(dataY - 1, V)\n",
        "```\n",
        "\n",
        "**Purpose:**\n",
        "Converts target word indices into one-hot encoded vectors.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* `dataY - 1` because tokenizer indices start from 1\n",
        "* `to_categorical()` â†’ convert to one-hot encoding\n",
        "\n",
        "**Viva Question:**\n",
        "\n",
        "* Why subtract 1 from `dataY`?\n",
        "* Why use one-hot encoding for output?\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§© **Cell 8: Train the Model**\n",
        "\n",
        "```python\n",
        "model.fit(dataX, y, epochs=500, verbose=0)\n",
        "```\n",
        "\n",
        "**Purpose:**\n",
        "Train CBOW model using context-target pairs.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* `epochs=500` â€“ trains the model 500 times\n",
        "* `verbose=0` â€“ hides training progress\n",
        "* Model learns word relationships\n",
        "\n",
        "**Viva Questions:**\n",
        "\n",
        "* What does one epoch mean?\n",
        "* Why choose 500 epochs?\n",
        "* What happens during model training?\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¶ POSSIBLE ADVANCED QUESTIONS (by Chief Examiner)\n",
        "\n",
        "| Question                                               | Answer                                                                             |\n",
        "| ------------------------------------------------------ | ---------------------------------------------------------------------------------- |\n",
        "| What is the main objective of CBOW?                    | To predict a target word given surrounding context words.                          |\n",
        "| Why is it called â€œBag of Wordsâ€?                       | Because order of context words doesnâ€™t matter â€” only their presence.               |\n",
        "| How are embeddings learned?                            | By minimizing loss between predicted and actual target word (via backpropagation). |\n",
        "| How can you visualize embeddings?                      | Using dimensionality reduction methods like PCA or t-SNE.                          |\n",
        "| What is the difference between one-hot and embeddings? | One-hot is sparse (high-dimensional), embeddings are dense (low-dimensional).      |\n",
        "| Can CBOW handle OOV (Out-of-Vocabulary) words?         | No, only words seen during training have embeddings.                               |\n",
        "| Why use softmax activation?                            | It converts output scores into probabilities for each word.                        |\n",
        "| What happens if window size increases too much?        | It may include irrelevant context and increase noise.                              |\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¶ SUMMARY (For Quick Revision)\n",
        "\n",
        "* **CBOW** = Predict center word from context\n",
        "* **Architecture:** Embedding â†’ Mean â†’ Softmax\n",
        "* **Loss:** Categorical Cross-Entropy\n",
        "* **Optimizer:** Adam\n",
        "* **Core goal:** Learn dense word embeddings\n",
        "* **Alternative:** Skip-Gram (reverse task)\n",
        "* **Applications:** Text recognition, NLP, translation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Perfect âœ… â€” since your practical exam is **â€œImplement the Continuous Bag of Words (CBOW) for text recognitionâ€**, Iâ€™ll now give a **line-by-line explanation** of *every code cell* in your notebook.\n",
        "\n",
        "This will be in **simple viva-ready format** â€” each code line explained clearly (what it does, why itâ€™s used, and how it fits in the CBOW model).\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§© **CELL 1 â€” Importing Required Libraries**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing import text\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "```\n",
        "\n",
        "### ğŸ”¹ Line-by-Line Explanation:\n",
        "\n",
        "1. `from tensorflow.keras.preprocessing import text`\n",
        "   â†’ Imports text preprocessing utilities (mainly for tokenization and text-to-sequence conversion).\n",
        "\n",
        "2. `from tensorflow.keras.preprocessing import sequence`\n",
        "   â†’ Used to manage sequences (padding/truncating sequences of words).\n",
        "\n",
        "3. `from tensorflow.keras.utils import pad_sequences`\n",
        "   â†’ Specifically used to make all input sequences of the same length (by adding zeros if needed).\n",
        "\n",
        "4. `from tensorflow.keras.utils import to_categorical`\n",
        "   â†’ Converts numerical labels into one-hot encoded form (used for categorical prediction).\n",
        "\n",
        "5. `import numpy as np`\n",
        "   â†’ NumPy provides support for arrays and numerical operations.\n",
        "\n",
        "6. `import pandas as pd`\n",
        "   â†’ Pandas is imported for data manipulation (though not heavily used here).\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§© **CELL 2 â€” Preparing the Text Dataset**\n",
        "\n",
        "```python\n",
        "#taking random sentences as data\n",
        "data = \"\"\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
        "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
        "\"\"\"\n",
        "dl_data = data.split()\n",
        "```\n",
        "\n",
        "### ğŸ”¹ Line-by-Line Explanation:\n",
        "\n",
        "1. `#taking random sentences as data`\n",
        "   â†’ This is a comment explaining that a random paragraph is used as a dataset.\n",
        "\n",
        "2. `data = \"\"\"Deep learning ...\"\"\"`\n",
        "   â†’ Defines a multiline string containing a paragraph of text about deep learning.\n",
        "\n",
        "3. `dl_data = data.split()`\n",
        "   â†’ Splits the paragraph into individual words based on spaces.\n",
        "   â†’ The result is a list of words, e.g. `[\"Deep\", \"learning\", \"also\", \"known\", ...]`.\n",
        "\n",
        "**Purpose:** This creates a simple corpus (collection of words) for training the CBOW model.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§© **CELL 3 â€” Tokenization and Vocabulary Creation**\n",
        "\n",
        "```python\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(dl_data)\n",
        "word2idx = tokenizer.word_index\n",
        "V = len(word2idx)\n",
        "```\n",
        "\n",
        "### ğŸ”¹ Line-by-Line Explanation:\n",
        "\n",
        "1. `tokenizer = text.Tokenizer()`\n",
        "   â†’ Creates a tokenizer object that will map each unique word to an integer.\n",
        "\n",
        "2. `tokenizer.fit_on_texts(dl_data)`\n",
        "   â†’ The tokenizer reads all the words and builds an internal vocabulary.\n",
        "   â†’ Each unique word is assigned a unique integer ID.\n",
        "\n",
        "3. `word2idx = tokenizer.word_index`\n",
        "   â†’ Creates a Python dictionary where keys = words and values = their integer indices.\n",
        "   Example: `{â€˜deepâ€™: 1, â€˜learningâ€™: 2, ...}`\n",
        "\n",
        "4. `V = len(word2idx)`\n",
        "   â†’ Calculates vocabulary size (`V`) = number of unique words.\n",
        "\n",
        "**Purpose:** Converts text words â†’ numerical IDs for neural network processing.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§© **CELL 4 â€” Creating Context and Target Word Pairs**\n",
        "\n",
        "```python\n",
        "window_size = 2\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(window_size, len(dl_data) - window_size):\n",
        "    context = []\n",
        "    for j in range(-window_size, window_size + 1):\n",
        "        if j != 0:\n",
        "            context.append(tokenizer.texts_to_sequences([dl_data[i + j]])[0][0])\n",
        "    dataX.append(context)\n",
        "    dataY.append(tokenizer.texts_to_sequences([dl_data[i]])[0][0])\n",
        "```\n",
        "\n",
        "### ğŸ”¹ Line-by-Line Explanation:\n",
        "\n",
        "1. `window_size = 2`\n",
        "   â†’ Defines how many words before and after the target word are used as context.\n",
        "   Example: if window=2 and target=\"learning\", context = [â€˜Deepâ€™, â€˜(alsoâ€™, â€˜asâ€™, â€˜deepâ€™]\n",
        "\n",
        "2. `dataX = []` and `dataY = []`\n",
        "   â†’ Initialize empty lists to store context (`dataX`) and target (`dataY`) pairs.\n",
        "\n",
        "3. `for i in range(window_size, len(dl_data) - window_size):`\n",
        "   â†’ Loops through each word, leaving space at start & end for context words.\n",
        "\n",
        "4. `context = []`\n",
        "   â†’ Temporary list to store context words for the current target word.\n",
        "\n",
        "5. `for j in range(-window_size, window_size + 1):`\n",
        "   â†’ Loops from `-2` to `+2` (for window size 2).\n",
        "\n",
        "6. `if j != 0:`\n",
        "   â†’ Skips the center word (because thatâ€™s the target).\n",
        "\n",
        "7. `context.append(tokenizer.texts_to_sequences([dl_data[i + j]])[0][0])`\n",
        "   â†’ Converts each context word into its numeric ID using tokenizer and appends to `context` list.\n",
        "\n",
        "8. `dataX.append(context)`\n",
        "   â†’ Adds the context (list of 4 integers) to the training input data list.\n",
        "\n",
        "9. `dataY.append(tokenizer.texts_to_sequences([dl_data[i]])[0][0])`\n",
        "   â†’ Adds the target wordâ€™s numeric ID to the output list.\n",
        "\n",
        "**Purpose:**\n",
        "Generates all possible `(context, target)` pairs for training the CBOW model.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§© **CELL 5 â€” Convert Lists to NumPy Arrays**\n",
        "\n",
        "```python\n",
        "dataX = np.array(dataX)\n",
        "dataY = np.array(dataY)\n",
        "```\n",
        "\n",
        "### ğŸ”¹ Line-by-Line Explanation:\n",
        "\n",
        "1. `dataX = np.array(dataX)`\n",
        "   â†’ Converts list of lists (contexts) into a 2D NumPy array for easier model input.\n",
        "\n",
        "2. `dataY = np.array(dataY)`\n",
        "   â†’ Converts list of target words into a NumPy array.\n",
        "\n",
        "**Purpose:** NumPy arrays are faster and compatible with Keras models.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§© **CELL 6 â€” Building the CBOW Neural Network Model**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Lambda, Dense\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(V+1, 10, input_length=window_size*2))\n",
        "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(10,)))\n",
        "model.add(Dense(V, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "```\n",
        "\n",
        "### ğŸ”¹ Line-by-Line Explanation:\n",
        "\n",
        "1. `from tensorflow.keras.models import Sequential`\n",
        "   â†’ Imports Keras Sequential model class (used to stack layers linearly).\n",
        "\n",
        "2. `from tensorflow.keras.layers import Embedding, Lambda, Dense`\n",
        "   â†’ Imports layers used:\n",
        "\n",
        "   * **Embedding:** converts word IDs to dense vectors\n",
        "   * **Lambda:** applies custom operations (like averaging)\n",
        "   * **Dense:** fully connected layer (output)\n",
        "\n",
        "3. `import tensorflow.keras.backend as K`\n",
        "   â†’ Imports Keras backend functions (for custom operations like mean).\n",
        "\n",
        "4. `model = Sequential()`\n",
        "   â†’ Initializes a sequential model (layer-by-layer neural network).\n",
        "\n",
        "5. `model.add(Embedding(V+1, 10, input_length=window_size*2))`\n",
        "   â†’ Adds an embedding layer:\n",
        "\n",
        "   * Input dimension: `V+1` (vocab size)\n",
        "   * Output dimension: 10 (embedding vector size)\n",
        "   * Input length: `window_size*2` (4 context words)\n",
        "\n",
        "6. `model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(10,)))`\n",
        "   â†’ Averages all 4 context word embeddings into a single vector (mean pooling).\n",
        "\n",
        "7. `model.add(Dense(V, activation='softmax'))`\n",
        "   â†’ Output layer with `V` neurons, one for each word in vocabulary.\n",
        "   â†’ `softmax` outputs probability for each word being the target.\n",
        "\n",
        "8. `model.compile(loss='categorical_crossentropy', optimizer='adam')`\n",
        "   â†’ Compiles model using categorical cross-entropy loss and Adam optimizer.\n",
        "\n",
        "**Purpose:**\n",
        "Defines the CBOW architecture:\n",
        "\n",
        "> Embedding â†’ Mean (context vector) â†’ Dense (Softmax output)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§© **CELL 7 â€” Converting Target Words to One-Hot Vectors**\n",
        "\n",
        "```python\n",
        "y = to_categorical(dataY - 1, V)\n",
        "```\n",
        "\n",
        "### ğŸ”¹ Line-by-Line Explanation:\n",
        "\n",
        "1. `dataY - 1`\n",
        "   â†’ Since tokenizer starts indexing from 1, we subtract 1 to shift to 0-based indexing.\n",
        "\n",
        "2. `to_categorical(..., V)`\n",
        "   â†’ Converts each target word ID into a one-hot encoded vector of size `V`.\n",
        "\n",
        "**Example:**\n",
        "If vocabulary size = 10 and target word index = 3,\n",
        "then one-hot vector = `[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]`\n",
        "\n",
        "**Purpose:**\n",
        "Transforms numerical targets into categorical format for softmax classification.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§© **CELL 8 â€” Training the Model**\n",
        "\n",
        "```python\n",
        "model.fit(dataX, y, epochs=500, verbose=0)\n",
        "```\n",
        "\n",
        "### ğŸ”¹ Line-by-Line Explanation:\n",
        "\n",
        "1. `model.fit(dataX, y, epochs=500, verbose=0)`\n",
        "   â†’ Trains the CBOW model for 500 epochs (iterations over dataset).\n",
        "   â†’ `verbose=0` hides training logs for cleaner output.\n",
        "\n",
        "**Purpose:**\n",
        "Model learns word embeddings and the relationship between context and target words.\n",
        "\n",
        "**Result:**\n",
        "Each word now has a learned 10-dimensional embedding that captures its meaning relative to others.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§  SUMMARY (Final Understanding)\n",
        "\n",
        "**CBOW Workflow in Your Code:**\n",
        "\n",
        "1. Import libraries â†’ preprocessing tools\n",
        "2. Prepare text dataset â†’ split into words\n",
        "3. Tokenize text â†’ get integer IDs\n",
        "4. Generate `(context, target)` pairs\n",
        "5. Build neural network â†’ Embedding â†’ Mean â†’ Softmax\n",
        "6. Train model â†’ learns word embeddings\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "1mKAoDERcjT-"
      },
      "id": "1mKAoDERcjT-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}